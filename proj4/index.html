<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS180 Project: Image Warping and Mosaicing</title>
    <meta name="author" content="Ziqian Luo">
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            background: linear-gradient(180deg, #ff7f50, #ff6347, #ffd700);
            background-size: 400% 400%;
            animation: gradientScroll 15s ease infinite;
        }
        @keyframes gradientScroll {
            0% { background-position: 0% 0%; }
            50% { background-position: 100% 100%; }
            100% { background-position: 0% 0%; }
        }
        .main-content {
            max-width: 1000px;
            width: 100%;
            margin: 20px;
            padding: 20px;
            background: #fff;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 20px;
            text-align: center;
            border-radius: 10px 10px 0 0;
        }
        h1 {
            font-size: 2.5em;
            margin-top: 0;
        }
        h2, h3 {
            margin: 20px 0;
            color: #333;
        }
        h3.author {
            color: #ccc;
            margin: 5px 0;
            font-weight: normal;
        }
        p, ul {
            text-align: justify;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            border-radius: 10px;
        }
        ul {
            text-align: left;
            padding: 0 20px;
        }
        a {
            color: #007acc;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        footer {
            background-color: #333;
            color: #fff;
            padding: 10px;
            text-align: center;
            border-radius: 0 0 10px 10px;
        }
        .content-table {
            margin: 20px auto;
            text-align: center;
            width: fit-content;
        }
        .content-table ul {
            list-style: none;
            padding: 0;
        }
        .content-table ul li {
            margin: 10px 0;
        }
    </style>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <div class="main-content">
        <header>
            <h1>CS180 Project 4: Auto Stitching Photo Mosaics</h1>
            <h3 class="author">Author: Ziqian Luo</h3>
        </header>

        <div class="content-table">
            <h3>Table of Contents</h3>
            <ul>
                <li><a href="#recover-homographies">Recover Homographies</a></li>
                <li><a href="#rectification">Image Rectification</a></li>
                <li><a href="#blend-mosaic">Blend the Images into a Mosaic</a></li>
                <li><a href="#corner-detection">Detect Corners using Harris</a></li>
                <li><a href="#ANMS">Adaptive Non-Maximal Suppression</a></li>
                <li><a href="#feature-extraction">Feature Descriptor Extraction</a></li>
                <li><a href="#feature-matching">Feature Matching using NNs and Lowe's Ratio Test</a></li>
                <li><a href="#ransac">RANSAC for Robust Feature Matching</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#bell-whistles1">Bell & Whistles: Multiscale Features</a></li>
                <li><a href="#bell-whistles2">Bell & Whistles: Rotation-Invariant Features</a></li>
                <li><a href="#bell-whistles3">Bell & Whistles: Auto Matching</a></li>
                <li><a href="#reflection">Reflection</a></li>
            </ul>
        </div>

        <h2 id="recover-homographies">Recover Homographies</h2>
        <p>To align the images correctly, I computed the homography matrix <code>H</code> that transforms the coordinates from one image to another. The homography relates two images with the equation <code>q = H p</code>, where <code>q</code> and <code>p</code> are points in homogeneous coordinates. I used at least four point correspondences to estimate <code>H</code>, and I solved for the matrix using Singular Value Decomposition (SVD) to ensure numerical stability and accuracy. Below is an example of how the homography matrix is defined and used:</p>

        <p>Let the homography matrix be
            \[ 
            H = \begin{bmatrix} h_1 & h_2 &
            h_3 \\ h_4 & h_5 & h_6 \\ h_7 & h_8 & 1
            \end{bmatrix} 
            \]
        </p>
        <p>Given a point \((x, y)\) in the source image and its corresponding point \((x', y')\) in the reference image, we can write the homography equation as
            \[
            \begin{bmatrix} x' \\ y' \\ w
            \end{bmatrix} = \begin{bmatrix} h_1 & h_2 & h_3 \\ h_4 & h_5
            & h_6 \\ h_7 & h_8 & 1 \end{bmatrix} \begin{bmatrix} x \\ y
            \\ 1 \end{bmatrix}
            \]
        </p> 
        <p>Given at least four point correspondences, we can write the above equation for each pair of points.
            Let  
            \[
            A = \begin{bmatrix} 
            x_1 & y_1 & 1 & 0 & 0 & 0 & -x'_1 x_1 & -x'_1 y_1 \\ 
            0 & 0 & 0 & x_1 & y_1 & 1 & -y'_1 x_1 & -y'_1 y_1 \\ 
            x_2 & y_2 & 1 & 0 & 0 & 0 & -x'_2 x_2 & -x'_2 y_2 \\ 
            0 & 0 & 0 & x_2 & y_2 & 1 & -y'_2 x_2 & -y'_2 y_2 \\ 
            & & & &  \vdots
            \end{bmatrix}
            \] 
        </p>
        <p> We can see that \[
            A\begin{bmatrix} h_1 \\ h_2 \\ h_3 \\ h_4 \\ h_5 \\ h_6 \\ h_7 \\ h_8 
            \end{bmatrix} =
            \begin{bmatrix}
            x'_1 \\ y'_1 \\ x'_2 \\ y'_2 \\ \vdots
            \end{bmatrix}
            \]
            To get \(H\), we can apply SVD on \(A\). The elements of the last column of \(V\) will be the elements of \(H\) since it corresponds to the smallest singular value.
        </p>

        <h2 id="rectification">Image Rectification</h2>
        <p>This process involved taking an image of a rectangular object and warping it such that the object appears perfectly rectangular in the final output. I used the computed homography from source points to reference points (I manually defined the reference points) to align the object correctly. I applied inverse warping along with interpolation to accurately map the pixels to their new locations. Additionally, I added an alpha channel to cover any unwanted parts of the image. Below is the original image and the rectified result:</p>
        <img src="results\rectification1.png" alt="rectification1">
        <img src="results\rectification2.png" alt="rectification2">

        <h2 id="blend-mosaic">Blend the Images into a Mosaic</h2>
        <p>Here are the steps to blend two images seamlessly:</p>

        <ol>
            <li><strong>Compute Homography Matrix</strong>:
                <ul>
                    <li>Uses corresponding points (<code>src_pts</code> and <code>ref_pts</code>) to calculate the homography matrix <code>H</code>.</li>
                </ul>
            </li>
            <li><strong>Determine Output Image Size</strong>:
                <ul>
                    <li>Transforms the source image corners and combines them with reference image corners to calculate dimensions and translation.</li>
                </ul>
            </li>
            <li><strong>Compute Translation Matrix</strong>:
                <ul>
                    <li>Generates a translation matrix <code>T</code> and determines the output image's height, width, and offsets.</li>
                </ul>
            </li>
            <li><strong>Apply Translation to Homography</strong>:
                <ul>
                    <li>Multiplies the translation matrix <code>T</code> with <code>H</code> to get <code>H_translated</code>.</li>
                </ul>
            </li>
            <li><strong>Warp Source Image</strong>:
                <ul>
                    <li>Applies <code>H_translated</code> to align the source image with the reference image.</li>
                </ul>
            </li>
            <li><strong>Create Canvas for Reference Image</strong>:
                <ul>
                    <li>Warps the reference image using <code>T</code> to fit onto the canvas.</li>
                </ul>
            </li>
            <li><strong>Extract Alpha Channels</strong>:
                <ul>
                    <li>Retrieves the alpha channels from both the canvas and warped source image.</li>
                </ul>
            </li>
            <li><strong>Compute Distance Transforms</strong>:
                <ul>
                    <li>Calculates the Euclidean distance transform for the alpha channels of both images, where each pixel's value represents the distance to the nearest edge of the image region.</li>
                </ul>
            </li>
            <li><strong>Create Blending Mask with Two-Band Blending</strong>:
                <ul>
                    <li>Uses a low-pass filter (Gaussian blur) to separate smooth regions and a high-pass filter to retain details.</li>
                    <li>For the high-pass blend, uses the maximum distance transform value between the two images to select the sharper details.</li>
                    <li>For the low-pass blend, combines the images using a weighted ratio of the distance transforms to smoothly blend overlapping areas.</li>
                </ul>
            </li>
            <li><strong>Create and Add Alpha Channel to Blended Image</strong>:
                <ul>
                    <li>Constructs an alpha channel for the final blended image by combining non-zero regions from both images and adds it to the RGB blended result.</li>
                </ul>
            </li>
        </ol>
        
            
        <p>Below is an example of the original images and the final mosaic:</p>
        <img src="results\src1.png" alt="Source Image 1">
        <img src="results\mosaic1.png" alt="Mosaic 1">
        <img src="results\src2.png" alt="Source Image 2">
        <img src="results\mosaic2.png" alt="Mosaic 2">
        <img src="results\src3.png" alt="Source Image 3">
        <img src="results\mosaic3.png" alt="Mosaic 3">

        <p> Here are the distance transforms, low-pass blended, and high-pass blended images for the second mosaic. </p>
        <img src="results\distances.png" alt="Distance Transform">
        <img src="results\blended.png" alt="Blended">

        <p> The images turned out fairly well with multiresolution blending.</p>

        <h2 id="corner-detection">Harris Corner Detection</h2>
        <p>I used sample code for Harris Corner Detection and, by adjusting the parameters, identified key corners in the image.</p>
        <img src="resultsb/harris.png" alt="Detected Corners">
        
        <h2 id="ANMS">Adaptive Non-Maximal Suppression (ANMS)</h2>
        <p>I implemented the Adaptive Non-Maximal Suppression (ANMS) algorithm to refine the detected corners by reducing their number. This algorithm sorts corners based on response values and iteratively removes those that are too close. The distance threshold is determined by the smallest distance to the nearest corner with a higher response value. Below is the result of applying ANMS to the Harris-detected corners:</p>
        <img src="resultsb/anms.png" alt="ANMS Result">
        
        <h2 id="feature-extraction">Feature Descriptor Extraction</h2>
        <p>Feature descriptors were obtained by extracting 40x40 patches around each corner, downsampling them to 8x8, and normalizing them to zero mean. Prior to extraction, the image was blurred to minimize noise.</p>
        <img src="resultsb/des.png" alt="Feature Descriptors">
        
        <h2 id="feature-matching">Feature Matching with NN and Lowe's Ratio Test</h2>
        <p>To match feature descriptors, I computed the 1-NN and 2-NN for each feature patch, representing them as 64-dimensional vectors and using Euclidean distance with a KD tree. Matching is established using Lowe's Ratio Test, where the distance to the 1-NN is divided by the distance to the 2-NN and considered valid if the ratio is below a specified threshold. Matches can be further refined by discarding those where the distance to the second nearest neighbor is below the average outlier distance, as suggested in the referenced paper.</p>
        <img src="resultsb/goodmatch.png" alt="Initial Feature Matches">
        <img src="resultsb/filteredmatch.png" alt="Refined Feature Matches">
        
        <h2 id="ransac">RANSAC for Robust Feature Matching</h2>
        <p>I applied the RANSAC algorithm, as covered in class, to filter out mismatches and identify the best homography matrix. This involves randomly selecting four matches to compute the homography, then applying it across matches and counting inliers within a specified threshold.</p>
        <img src="resultsb/ransacmatch.png" alt="Inliers after RANSAC">
        
        <h2 id="results">Final Results</h2>
        <p>Below are the results comparing manual and automated methods:</p>
        <img src="results/mosaic1.png" alt="Manual Match 1">
        <img src="resultsb/mosaic1.png" alt="Automated Match 1">
        <img src="results/mosaic2.png" alt="Manual Match 2">
        <img src="resultsb/mosaic2.png" alt="Automated Match 2">
        <img src="results/mosaic3.png" alt="Manual Match 3">
        <img src="resultsb/mosaic3.png" alt="Automated Match 3">
        
        <h2 id="bell-whistles1">Bell & Whistles: Multiscale Feature Extraction</h2>
        <p>I implemented multiscale feature extraction by applying Gaussian blurring to the image and downsampling it by a factor of two. Features were then extracted from each scale and combined to enhance the final feature descriptors, showing improvements over the single-scale approach.</p>
        <img src="resultsb/bwmulti0.png" alt="Multiscale Features">
        <img src="resultsb/bwmulti1.png" alt="Multiscale Features">
        <img src="resultsb/bwmulti2.png" alt="Multiscale Features">
        <img src="resultsb/bwmulti3.png" alt="Multiscale Features">
        <img src="resultsb/bwmatch.png" alt="Multiscale Matched Features">
        
        <h2 id="bell-whistles2">Bell & Whistles: Rotation-Invariant Features</h2>
        <p>To make the feature extraction rotation-invariant, I computed the gradient at each corner and aligned the image to extract features that remain stable under rotation.</p>
        <img src="resultsb/bwarrow.png" alt="Rotation-Invariant Feature Visualization">
        <img src="resultsb/bwpatch.png" alt="Rotation-Invariant Feature Patches">
        <img src="resultsb/bwdes.png" alt="Rotation-Invariant Descriptors">
        
        <h2 id="bell-whistles3">Bell & Whistles: Automatic Panorama Matching</h2>
        <p>To create an automated panorama detection process for unordered images, I began by calculating robust correspondences between each image pair using RANSAC. Images with enough correspondences were grouped as part of the same panorama, forming nodes in a graph where each connected component represents a unique panorama. Within each component, I used a maximum spanning tree (MST) to prioritize the strongest connections, reducing redundancy by minimizing the number of image pairs needed to stitch the panorama. To determine the ideal base image, I calculated the betweenness centrality of each node in the MST, selecting the image with the highest centrality as the anchor, thereby minimizing transformation complexity during stitching. This approach builds on techniques from <a href="https://inst.eecs.berkeley.edu/~cs180/fa23/upload/files/proj4B/alec.li/">Alec Li's Project 4B writeup</a>.</p>

        <img src="resultsb/bwimages.png" alt="Detected Image Pairs for Panorama">
        <img src="resultsb/bwgraph.png" alt="Panorama Graph Structure">
        <img src="resultsb/bwc1.png" alt="Automated Panorama Stitching">
        <img src="resultsb/bwc2.png" alt="Automated Panorama Stitching">
        <img src="resultsb/bwc3.png" alt="Automated Panorama Stitching">
        
        <h2 id="reflection">Reflection</h2>
        <p>This project provided a valuable learning experience. It strengthened my understanding of image warping, stitching, and feature extraction, while also showing me how useful graph algorithms like maximum spanning trees and centrality can be for organizing and simplifying panorama detection. I learned the importance of stability in homography calculations and saw the benefits of using multiresolution blending along with features that are both scale- and rotation-invariant. Working through the challenges of building and refining these techniques was both demanding and rewarding, and I’m excited to apply them in future projects.</p>
        
        <footer>&copy; 2024 Ziqian Luo. No Rights Reserved.</footer>
    </div>
</body>
</html>
